"""
Utilities for working with files.
"""

from __future__ import annotations
import os
import pickle as pkl
import json
from copy import deepcopy
from collections import defaultdict
from collections.abc import Callable, Iterator
from typing import Any, cast

import torch
import numpy as np

from . import configs, utils


def save_experiment(
    exp_dict: dict,
    results_dir: str = "results",
    results: Any | None = None,
    metrics: dict[str, Any] | None = None,
    model: Any | None = None,
):
    """Save results of the experiment corresponding to the given dictionary.

    Params:
        exp_dict: experiment dictionary.
        results_dir: base directory for experimental results.
        results: the raw return value of the experiment.
        metrics: (optional) dictionary of metrics collected during the
        experiment.
        model: (optional) the optimized model to be serialized.
    """

    hash_id = configs.hash_dict(exp_dict)
    path = os.path.join(results_dir, hash_id)

    # make directory if it doesn't exist.
    os.makedirs(path, exist_ok=True)

    # write the experiment configuration to disk.
    with open(os.path.join(path, "config.json"), "w", encoding="utf-8") as f:
        json.dump(exp_dict, f)

    if results is not None:
        with open(os.path.join(path, "return_value.pkl"), "wb") as f:
            pkl.dump(results, f)

    if metrics is not None:
        with open(os.path.join(path, "metrics.pkl"), "wb") as f:
            pkl.dump(metrics, f)

    if model is not None:
        # save a PyTorch model using torch builtins.
        if isinstance(model, torch.nn.Module):
            torch_path = os.path.join(path, "model.pt")
            model = cast(torch.nn.Module, model)

            # extract the state dictionary and save the model.
            state_dict = model.state_dict()
            torch.save(state_dict, torch_path)
        else:
            # pickle the model as usual.
            pkl_path = os.path.join(path, "model.pkl")
            with open(pkl_path, "wb") as f:
                pkl.dump(model, f)

    return results


def load_experiment(
    exp_dict: dict | None = None,
    hash_id: str | None = None,
    results_dir: list[str] | str = "results",
    load_metrics: bool = False,
    load_model: bool = False,
) -> dict[str, Any]:
    """Load results of from an experiment.

    One of `exp_dict` or `has_id` must be provided in order to load
    the experiment.

    Params:
        exp_dict: experiment dictionary.
        hash_id: the id generated by hashing an experiment dictionary.
        results_dir: base directory or list of base directories for
        experimental results.
        load_metrics: whether or not to load metrics from the experiment.
        load_model: whether or not to load a model associated with the
        experiment.

    Returns:
        Dictionary containing results. It is indexed by 'return_value' and
        (optionally) by 'metrics', 'model'.
    """

    if hash_id is None:
        if exp_dict is None:
            raise ValueError("One of 'exp_dict' or 'hash_id' must not be 'None'.")

        hash_id = configs.hash_dict(exp_dict)

    results = {}

    success = False
    for src in utils.as_list(results_dir):
        path = os.path.join(src, hash_id)
        if os.path.exists(path):
            success = True
            break

    if not success:
        raise ValueError(f"Cannot find experiment {exp_dict} in one of {results_dir}!")

    try:
        with open(os.path.join(path, "return_value.pkl"), "rb") as f:
            results["return_value"] = pkl.load(f)
    except FileNotFoundError:
        results["return_value"] = None

    if load_metrics:
        with open(os.path.join(path, "metrics.pkl"), "rb") as f:
            results["metrics"] = pkl.load(f)

    if load_model:
        pkl_path = os.path.join(path, "model.pkl")
        torch_path = os.path.join(path, "model.pt")

        if os.path.exists(pkl_path):
            with open(pkl_path, "rb") as f:
                results["model"] = pkl.load(f)
        elif os.path.exists(torch_path):
            results["model"] = torch.load(torch_path)

    return results


def load_metric_grid(
    grid: dict,
    results_dir: list[str] | str = "results",
    processing_fn: Callable | None = None,
    silent_fail: bool = False,
) -> dict:
    """Load metrics according to a supplied grid of experiment dictionaries.

    Parameters:
        grid: a grid of experiment dictionaries. See 'configs.make_grid'.
        results_dir: base directory or list of base directories for
            experimental results.
        processing_fn: (optional) a function to call on each experiment to
            process the results.
        silent_fail: suppress exceptions when some experiments can't be loaded.

    Returns:
        nested dictionary with the same "shape" as 'grid' containing the
        loaded metrics.
    """

    results_grid = deepcopy(grid)
    results_grid = defaultdict(
        lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))
    )

    if processing_fn is None:

        def processing_fn(vals, _):
            return vals

    for row in grid.keys():
        for metric_name in grid[row].keys():
            for line in grid[row][metric_name].keys():
                for repeat in grid[row][metric_name][line].keys():
                    for variation in grid[row][metric_name][line][repeat].keys():
                        try:
                            metrics = load_experiment(
                                exp_dict=grid[row][metric_name][line][repeat][
                                    variation
                                ],
                                results_dir=results_dir,
                                load_metrics=True,
                                load_model=False,
                            )["metrics"]
                        except Exception as e:
                            if silent_fail:
                                continue

                            raise e

                        vals = utils.as_list(metrics[metric_name])
                        vals = np.nan_to_num(vals, nan=0)

                        results_grid[row][metric_name][line][repeat][
                            variation
                        ] = processing_fn(
                            vals,
                            (row, metric_name, line, repeat, variation),
                        )

    return results_grid


def optimize_over_variations(
    metric_grid: dict,
    target_metric: str | None = None,
    maximize_target: bool = False,
) -> dict:
    optimized_grid: dict = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))

    for row in metric_grid.keys():
        for metric_name in metric_grid[row].keys():
            for line in metric_grid[row][metric_name].keys():
                for repeat in metric_grid[row][metric_name][line].keys():
                    variations = list(
                        metric_grid[row][metric_name][line][repeat].keys()
                    )

                    if target_metric is None:
                        assert len(variations) == 1
                        best_variation = variations[0]
                    else:
                        target_vals = [
                            metric_grid[row][target_metric][line][repeat][var][-1]
                            for var in variations
                        ]
                        target_vals = np.nan_to_num(target_vals, nan=np.inf)

                        index = (
                            np.argmax(target_vals)
                            if maximize_target
                            else np.argmin(target_vals)
                        )
                        best_variation = variations[index]

                    optimized_grid[row][metric_name][line][repeat] = metric_grid[row][
                        metric_name
                    ][line][repeat][best_variation]

    return optimized_grid


def compute_metrics(
    metric_grid: dict,
    metric_fn: Callable | None = None,
    x_key: str | None = None,
    x_vals: list | np.ndarray | None = None,
) -> dict:
    """Load metrics according to a supplied grid of experiment dictionaries.

    Params:
        metric_grid: a grid of experiment dictionaries with loaded metrics.
        metric_fn: (optional) function to process metrics after they are
        loaded.
        x_key: (optional) metric to use as "x axis" information (e.g. "time").
        x_vals: (optional) x-axis values. Cannot be supplied at the same time
        as `x_key`.

    Returns:
        Nested dictionary with the same nested structure as `grid` containing
        the loaded metrics.
    """

    # both cannot be provided.
    assert x_key is None or x_vals is None

    results_grid = deepcopy(metric_grid)

    if metric_fn is None:

        def metric_fn(x):
            return x

    for row in metric_grid.keys():
        for metric_name in metric_grid[row].keys():
            for line in metric_grid[row][metric_name].keys():
                results = []
                repeats = metric_grid[row][metric_name][line].keys()
                for repeat in repeats:
                    vals = metric_grid[row][metric_name][line][repeat]

                    results.append(vals)

                results_grid[row][metric_name][line] = metric_fn(
                    results, metric_name=metric_name
                )

        if x_key is not None or x_vals is not None:
            for metric_name in metric_grid[row].keys():
                for line in metric_grid[row][metric_name].keys():
                    if x_key is not None:
                        results_grid[row][metric_name][line]["x"] = results_grid[row][
                            x_key
                        ][line]["center"]
                    elif x_vals is not None:
                        results_grid[row][metric_name][line]["x"] = x_vals

    return results_grid


def load_and_clean_experiments(
    exp_configs: list[dict[str, Any]],
    results_dir: str | list[str],
    metrics: list[str],
    row_key: Any | Iterator[Any],
    line_key: Any | Iterator[Any],
    repeat_key: Any | Iterator[Any],
    variation_key: Any | Iterator[Any],
    metric_fn: Callable = utils.final_metrics,
    target_metric: str | None = None,
    maximize_target: bool = False,
    keep: list[tuple[Any, Any]] = [],
    remove: list[tuple[Any, Any]] = [],
    filter_fn: Callable | None = None,
    transform_fn: Callable | None = None,
    processing_fns: list[
        Callable[[dict[str, np.ndarray], tuple], dict[str, np.ndarray]]
    ] = [],
    x_key: str | None = None,
    x_vals: list | np.ndarray | None = None,
    silent_fail: bool = False,
):
    """Load and clean a grid of experiments for plotting.

    This is a convenience function which composes other built-ins into one
    block.

    Params:
        exp_configs: list of experiment configuration objects.
        results_dir: a base directory or list of base directories where the
            results are stored.
        metrics: list of strings identifying different metrics to load.
        row_key: key (or iterable of keys) for which distinct values in the
            experiment dictionaries are to be split into different rows.
        line_key: key (or iterable of keys) for which distinct values in the
            experiment dictionaries are to be split into different lines.
        repeat_key: key (or iterable of keys) for which distinct values in
            the experiment dictionaries are to be *averaged* over in the plot.
        metric_fn: (optional) function to process metrics after they are
            loaded.
        keep: A list of key-value pairs to retain with the form
            `[(key, values)]`. Each `key` is either a singleton key for the
            top-level dictionary or an iterable of keys indexing into nested
            dictionaries, while `values` is either singleton or list of values.
        remove: A list of key-value pairs to filter with the form
            `[(key, values)]`. Arguments should take the same form as `keep`.
        filter_fn: An additional filter to run on each dictionary. It should
            return `True` for experiments to retain and `False` otherwise.
        transform_fn: a function to call on the complete metric grid.
        processing_fns: a list of functions to be called on the leaves of the
            loaded experiment grid. Order matters.
        x_key: a key which indexes into the values which should be used as the
            x-axis.
        x_vals: x-axis values. Cannot be supplied at the same time as 'x_key'.
        silent_fail: suppress exceptions when some experiments can't be loaded.

    Returns:
        Nested dictionary containing loaded and cleaned experiment results.
    """

    exp_list = configs.expand_config_list(exp_configs)
    filtered_exp_list = configs.filter_dict_list(
        exp_list,
        keep=keep,
        remove=remove,
        filter_fn=filter_fn,
    )

    # make sure we load the metric associated with the x-axis.
    added_x = False

    if x_key is not None and x_key not in metrics:
        added_x = True
        metrics = metrics + [x_key]

    exp_grid = configs.make_metric_grid(
        filtered_exp_list,
        metrics,
        row_key,
        line_key,
        repeat_key,
        variation_key,
    )

    def call_on_fn(exp, key):
        for fn in processing_fns:
            exp = fn(exp, key)
        return exp

    metric_grid = load_metric_grid(
        exp_grid,
        results_dir,
        processing_fn=call_on_fn,
        silent_fail=silent_fail,
    )

    metric_grid = optimize_over_variations(
        metric_grid,
        target_metric,
        maximize_target,
    )

    if transform_fn is not None:
        metric_grid = transform_fn(metric_grid)

    metric_grid = compute_metrics(
        metric_grid,
        metric_fn=metric_fn,
        x_key=x_key,
        x_vals=x_vals,
    )

    # drop x if necessary
    if added_x:
        for _, val in metric_grid.items():
            # remove x_key
            val.pop(x_key)

    return metric_grid
